{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"trusTEr - A trusting TE cluster analysis Version 0.1.1, written in Python 3.6.6 Takes fastq files from 10x single cell RNA sequencing, clusters cells using Seurat, and can be used to produce read count matrices in a cluster level. You can also quantify reads per cluster having predefined clusters. Requirements TrusTEr depends on several external software. We provide a Docker container and a conda environment for a quick-start. TrusTEr requires: Cellranger R (version 3.6) Seurat TEtranscripts STAR aligner subset-bam and bamtofastq from 10x Genomics Velocyto The package has been tested in Unix systems only and supports only SLURM job submissions. How to install Just the modules If you fulfill the requirements, you can install via pip: pip install truster With Docker container With conda environment Introduction As single cell technologies haven't developed to the point where we can get the needed sequencing depth to study transposable elements expression, trusTEr seeks produce more reliable results by combining reads from closely related cells to gain information on a cell type level. Structure trusTEr uses composition assiciation to relate three main classes: Experiment: Includes information about the experiment as a whole. This is the main object you will be working with. Name is required, description is optional. Register samples by providing a path to its fastq files. Sample: Created by giving a path to fastq files Name and ID required. Cluster: Created by running getClusters() or mergeSamples() functions (Or setClustersOutdir() or setMergeSamplesOutdir() ). Functionality and workflow This package is meant to be run with the following workflow: Depending on the object type, you have access to different functions to go through these steps. Experiment is the main object one would work with. Here you will registerSample() or registerSamplesFromPath() . An object of type Sample has access to step 7 and some handy wrappers to use cellRanger , perform clustering with Seurat and run and plot RNA velocity. These functions can be called for all registered samples from your object of type Experiment (See quantify() , getClustersAllSamples() , velocityAllSamples() , plotVelocityAllSamples() ). The need for the class Experiment is clearer once the user wants to merge samples (See mergeSamples() ) or to run the same workflow for all the samples' clusters. The user won't work directly with an object of type Cluster , but this class includes all the functions needed to go through steps 1-6 of the workflow. Instead of running this pipeline individually for each cluster, one can run the workflow for each cluster of each registered sample using the Experiment function processClusters(mode = \"perSample\", ...) . One can also partition the workflow and run step by step in all registered samples or in a combined clustering using the transitioning functions of Experiment to call the needed functions in the class Cluster (See tsvToBamClusters() , filterUMIsClusters() , bamToFastqClusters() , concatenateLanesClusters() , mergeClusters() , mapClusters() , TEcountsClusters() , normalizeTECounts() ). Running on a server At the moment of construction of an Experiment object, you can declare slurmPath and a modulePath . These files will be checked whenever a function can be run as an sbatch job. For the moment, trusTEr only works with slurm systems. You need to create two json files: 1. slurmPath Declaring the sbatch options per function. You need to declare a field per function listed that you will use: quantify getClusters mergedClusters tsvToBam filterUMIs bamToFastq mapCluster TEcount velocity As a brief example: { \"__default__\" : { \"account\" : \"myaccount\", \"time\" : \"00:15:00\", \"nodes\" : 1, \"partition\" : \"dell\", \"output\" : \"%j.out\", \"error\" : \"%j.err\", \"job-name\" : \"%j\" }, \"quantify\" : { \"account\" : \"myaccount\", \"time\" : \"10:00:00\", \"nodes\" : 1, \"tasks-per-node\" : 20, \"partition\" : \"dell\", \"output\" : \"%j.quantify.out\", \"error\" : \"%j.quantify.err\", \"job-name\" : \"%j.quantify\" }, \"getClusters\" : {...}, \"mergeSamples\" : {...}, \"tsvToBam\" : {...}, \"filterUMIs\" : {...}, \"bamToFastq\" : {...}, \"mapCluster\" : {...}, \"TEcount\" : {...}, \"velocity\" : {...} } 2. modulesPath In a slurm system, many times you need to load modules for a software to be available. In this json file you declare the name of the needed modules. You need to declare a field per function listed that you will use: quantify getClusters mergedClusters tsvToBam filterUMIs bamToFastq mapCluster TEcount velocity normalizeTEcounts plotTEexpression As an example: { \"quantify\":[\"cellranger/3.1.0\"], \"velocity\":[\"GCC/7.3.0-2.30\", \"SAMtools/1.9\", \"velocyto/0.17.17\"], \"getClusters\":[\"GCC/9.3.0\", \"OpenMPI/4.0.3\", \"Seurat/3.1.5-R-4.0.0\"], \"mergeSamples\":[\"GCC/9.3.0\", \"OpenMPI/4.0.3\", \"Seurat/3.1.5-R-4.0.0\"], \"tsvToBam\":[\"subset-bam/1.0\"], \"filterUMIs\":[\"GCC/7.3.0-2.30\", \"OpenMPI/3.1.1\", \"Pysam/0.15.1-Python-3.6.6\"], \"bamToFastq\":[\"bamtofastq/1.2.0\"], \"mapCluster\":[\"GCC/5.4.0-2.26\", \"OpenMPI/1.10.3\", \"STAR/2.6.0c\"], \"TEcount\":[\"icc/2018.1.163-GCC-6.4.0-2.28\", \"OpenMPI/2.1.2\", \"TEToolkit/2.0.3-Python-2.7.14\"], \"normalizeTEcounts\":[\"GCC/9.3.0\", \"OpenMPI/4.0.3\", \"Seurat/3.1.5-R-4.0.0\"], \"plotTEexpression\":[\"GCC/9.3.0\", \"OpenMPI/4.0.3\", \"Seurat/3.1.5-R-4.0.0\"] } If you don't need to load any modules for a software to be available, you can leave the respective list empty. You can read the functions' documentation and some tutorials at https://ra7555ga-s.github.io/truster/","title":"Introduction"},{"location":"#truster-a-trusting-te-cluster-analysis","text":"","title":"trusTEr - A trusting TE cluster analysis"},{"location":"#version-011-written-in-python-366","text":"Takes fastq files from 10x single cell RNA sequencing, clusters cells using Seurat, and can be used to produce read count matrices in a cluster level. You can also quantify reads per cluster having predefined clusters.","title":"Version 0.1.1, written in Python 3.6.6"},{"location":"#requirements","text":"TrusTEr depends on several external software. We provide a Docker container and a conda environment for a quick-start. TrusTEr requires: Cellranger R (version 3.6) Seurat TEtranscripts STAR aligner subset-bam and bamtofastq from 10x Genomics Velocyto The package has been tested in Unix systems only and supports only SLURM job submissions.","title":"Requirements"},{"location":"#how-to-install","text":"","title":"How to install"},{"location":"#just-the-modules","text":"If you fulfill the requirements, you can install via pip: pip install truster","title":"Just the modules"},{"location":"#with-docker-container","text":"","title":"With Docker container"},{"location":"#with-conda-environment","text":"","title":"With conda environment"},{"location":"#introduction","text":"As single cell technologies haven't developed to the point where we can get the needed sequencing depth to study transposable elements expression, trusTEr seeks produce more reliable results by combining reads from closely related cells to gain information on a cell type level.","title":"Introduction"},{"location":"#structure","text":"trusTEr uses composition assiciation to relate three main classes: Experiment: Includes information about the experiment as a whole. This is the main object you will be working with. Name is required, description is optional. Register samples by providing a path to its fastq files. Sample: Created by giving a path to fastq files Name and ID required. Cluster: Created by running getClusters() or mergeSamples() functions (Or setClustersOutdir() or setMergeSamplesOutdir() ).","title":"Structure"},{"location":"#functionality-and-workflow","text":"This package is meant to be run with the following workflow: Depending on the object type, you have access to different functions to go through these steps. Experiment is the main object one would work with. Here you will registerSample() or registerSamplesFromPath() . An object of type Sample has access to step 7 and some handy wrappers to use cellRanger , perform clustering with Seurat and run and plot RNA velocity. These functions can be called for all registered samples from your object of type Experiment (See quantify() , getClustersAllSamples() , velocityAllSamples() , plotVelocityAllSamples() ). The need for the class Experiment is clearer once the user wants to merge samples (See mergeSamples() ) or to run the same workflow for all the samples' clusters. The user won't work directly with an object of type Cluster , but this class includes all the functions needed to go through steps 1-6 of the workflow. Instead of running this pipeline individually for each cluster, one can run the workflow for each cluster of each registered sample using the Experiment function processClusters(mode = \"perSample\", ...) . One can also partition the workflow and run step by step in all registered samples or in a combined clustering using the transitioning functions of Experiment to call the needed functions in the class Cluster (See tsvToBamClusters() , filterUMIsClusters() , bamToFastqClusters() , concatenateLanesClusters() , mergeClusters() , mapClusters() , TEcountsClusters() , normalizeTECounts() ).","title":"Functionality and workflow"},{"location":"#running-on-a-server","text":"At the moment of construction of an Experiment object, you can declare slurmPath and a modulePath . These files will be checked whenever a function can be run as an sbatch job. For the moment, trusTEr only works with slurm systems. You need to create two json files:","title":"Running on a server"},{"location":"#1-slurmpath","text":"Declaring the sbatch options per function. You need to declare a field per function listed that you will use: quantify getClusters mergedClusters tsvToBam filterUMIs bamToFastq mapCluster TEcount velocity As a brief example: { \"__default__\" : { \"account\" : \"myaccount\", \"time\" : \"00:15:00\", \"nodes\" : 1, \"partition\" : \"dell\", \"output\" : \"%j.out\", \"error\" : \"%j.err\", \"job-name\" : \"%j\" }, \"quantify\" : { \"account\" : \"myaccount\", \"time\" : \"10:00:00\", \"nodes\" : 1, \"tasks-per-node\" : 20, \"partition\" : \"dell\", \"output\" : \"%j.quantify.out\", \"error\" : \"%j.quantify.err\", \"job-name\" : \"%j.quantify\" }, \"getClusters\" : {...}, \"mergeSamples\" : {...}, \"tsvToBam\" : {...}, \"filterUMIs\" : {...}, \"bamToFastq\" : {...}, \"mapCluster\" : {...}, \"TEcount\" : {...}, \"velocity\" : {...} }","title":"1. slurmPath"},{"location":"#2-modulespath","text":"In a slurm system, many times you need to load modules for a software to be available. In this json file you declare the name of the needed modules. You need to declare a field per function listed that you will use: quantify getClusters mergedClusters tsvToBam filterUMIs bamToFastq mapCluster TEcount velocity normalizeTEcounts plotTEexpression As an example: { \"quantify\":[\"cellranger/3.1.0\"], \"velocity\":[\"GCC/7.3.0-2.30\", \"SAMtools/1.9\", \"velocyto/0.17.17\"], \"getClusters\":[\"GCC/9.3.0\", \"OpenMPI/4.0.3\", \"Seurat/3.1.5-R-4.0.0\"], \"mergeSamples\":[\"GCC/9.3.0\", \"OpenMPI/4.0.3\", \"Seurat/3.1.5-R-4.0.0\"], \"tsvToBam\":[\"subset-bam/1.0\"], \"filterUMIs\":[\"GCC/7.3.0-2.30\", \"OpenMPI/3.1.1\", \"Pysam/0.15.1-Python-3.6.6\"], \"bamToFastq\":[\"bamtofastq/1.2.0\"], \"mapCluster\":[\"GCC/5.4.0-2.26\", \"OpenMPI/1.10.3\", \"STAR/2.6.0c\"], \"TEcount\":[\"icc/2018.1.163-GCC-6.4.0-2.28\", \"OpenMPI/2.1.2\", \"TEToolkit/2.0.3-Python-2.7.14\"], \"normalizeTEcounts\":[\"GCC/9.3.0\", \"OpenMPI/4.0.3\", \"Seurat/3.1.5-R-4.0.0\"], \"plotTEexpression\":[\"GCC/9.3.0\", \"OpenMPI/4.0.3\", \"Seurat/3.1.5-R-4.0.0\"] } If you don't need to load any modules for a software to be available, you can leave the respective list empty. You can read the functions' documentation and some tutorials at https://ra7555ga-s.github.io/truster/","title":"2. modulesPath"},{"location":"workflow/","text":"A workflow example Let's begin with a simple example. Let's say you have two samples of the same tissue, they were sequenced in the same sequencing run and you want to get transposon expression per cell type found in the tissue. Registering samples The first step is to have the fastq files in a file system such as: data \u2502 \u2514\u2500\u2500\u2500sample1 \u2502 \u2502 sample1Name_L001_I1_001.fastq.gz \u2502 \u2502 sample1Name_L001_R1_001.fastq.gz \u2502 \u2502 sample1Name_L001_R2_001.fastq.gz \u2502 \u2502 sample1Name_L002_I1_001.fastq.gz \u2502 \u2502 sample1Name_L002_R1_001.fastq.gz \u2502 \u2514\u2500 sample1Name_L002_R2_001.fastq.gz \u2514\u2500\u2500\u2500sample2 \u2502 sample2Name_L001_I1_001.fastq.gz \u2502 sample2Name_L001_R1_001.fastq.gz \u2502 sample2Name_L001_R2_001.fastq.gz \u2502 sample2Name_L002_I1_001.fastq.gz \u2502 sample2Name_L002_R1_001.fastq.gz \u2514\u2500 sample2Name_L002_R2_001.fastq.gz We can then register the samples in an experiment object import truster raw_path = [\"/path/to/data/\"] example = truster.experiment(name = \"example\") example.registerSamplesFromPath(raw_path) This will look for this structure of files in the path and create an object \"sample\" per subdirectory in the path, the object will have the name of the subdirectories as ID (i.e. sample1) and the name of the files (i.e. sample1Name) as sample name. The \"sample\" objects are contained to the object \"experiment\" which here we called example . If there was any sample in the data/ folder you didn't want to include, you can unregister it with example.unregisterSample(sampleId = \"sample1\") Similarly, you can register individual samples as example.registerSample(sampleId = \"sample3\", sampleName = \"sample3Name\", rawPath = \"/path/to/sample3/fastqfiles/\") Mapping and quantification We now need to run cellranger. TrusTEr has a wrapper for it and can be used to run it for all samples registered in an experiment. In this example: crIndex = '/path/to/cellranger/index' outdir = '/output/path' example.quantify(crIndex, outdir) If you already have the output from cellranger, feel free to set the quantification outdir with example.setQuantificationOutdir(outdir) Note: Before continuing, don't forget to check the quality of your samples! Clustering Clustering samples is not a trivial step. It requires knowledge of the tissue being analyzed. We wrapped a very basic clustering with Seurat as a reference to make the process smoother for the user, but be aware that there are plenty of parameters that you could tweak to get the perfect clustering for your particular data. Per sample We can get a clustering of each sample by typing clusters_dir = 'wherever/you/want/the/output' example.getClustersAllSamples(clusters_dir) This will create the clusters_dir and a subfolder per sample (named with the sampleIds ). Each of these subfoldes will contain a tsv file per cluster found on the sample. The tsv files contain the cells barcodes that form that cluster. You will also get an rds file in the output directory with the Seurat object of each of your samples. If you already have a clustering of your preference, please produce the required tsv files and set the clusters directory as: example.setClustersOutdir(processClustersOutdir = outdir) We could add a function in the near future that takes rds with a Seurat object and produces the tsv files in the file structure we need it. Merged samples In some experiments, such as this one where the samples are from the same tissue, it's interesting to combine the samples and get a merged clustering. This can be achieved as mergedsamples_dir = 'wherever/you/want/the/output' example.mergeSamples(mergedsamples_dir) Similarly to the clusters per sample, if you already have a clustering you want to use, you could set the directory where you contain the tsv files with the cell barcodes as example.setMergeSamplesOutdir(outdir) If you are going to merge the Seurat objects yourself, we ask you to name the merged tsv files as [sampleId]_merged.clusters_[cluster number].tsv Transposon expression Once we have the cellranger output, and the clusters that we want to check for transposon expression, we can go ahead and run trusTEr as a pipeline or step by step. This part is a set of seven steps: Extract cell barcodes from the bam files Filter for unique UMIs Convert bam to fastq files Concatenate lanes from the step #3 output Merge samples in one cluster Map fastq files TE quantification TE quantification normalization As a pipeline You can run everything at once. You will need to provide a gene and a TE GTF (as required by TEtranscripts output. You can check their downloads here), the path to the STAR index you want to use, the mode meaning the type of clustering you are using (\"merged\" or \"perSample\"), and the output directory. output = 'wherever/you/want/the/output' gene_gtf = 'path/to/gene.gtf' te_gtf = 'path/to/te.gtf' star_index = 'path/to/star/index' example.processClusters(mode = \"merged\", outdir = output, geneGTF = gene_gtf, teGTF = te_gtf, starIndex = star_index) Step by step If you want to wait a bit and check the output of each step of the pipeline, you an of course run it step by step. 1) Extract clusters Extract cell barcodes from the bam files typing example.tsvToBamClusters(mode = \"merged\", outdir) This will create a directory inside the outdir named tsvToBam/ . This directory will contain subdirectories one per sample, containing the bam files for each of their clusters. 2) Filter UMIs Because PCR duplication is a needed step for 10x RNA sequencing, we need to make sure there are no duplicated molecules when quantifying repetitive elements. We ensure the molecules are unique by keeping only reads with a unique combination of cell barcode, UMI and sequence. To filter duplicates in our merged clustering bam files: example.filterUMIsClusters(\"merged\", outdir) This will create a directory inside the outdir named filterUMIs/ . Similarly to tsvToBam/ it contains a subdirectory per sample, each containing the filtered bam file. 3) Convert bam to fastq This is just a file conversion step dependent on bamtofastq from 10x Genomics. example.bamToFastqClusters(\"merged\", outdir) Will create a directory inside the outdir named bamToFastq/ . Each sample subdirectory has subdirectories for each cluster which contain the fastq files of the clusters in different lanes (L00[1-9]). 4) Concatenate lanes We will take the sequence fastq files ( *_R2_001.fastq.gz ) and concatenate them. This will produce a bulk file for each cluster. example.concatenateLanesClusters(\"merged\", outdir) Again, this will create a directory inside the outdir named concatenateLanes/ which will include sample subdirectories containing the concatenated fastq files per cluster. 5) Map fastq files We will now map these files using STAR. Again, this is just a basic wrapper using TEtranscript's authors recommendations, but if you need to tweak more parameters, feel free to do this step yourself and then setting the output directory to continue. You will need a gene GTF and a STAR index. output = 'wherever/you/want/the/output' gene_gtf = 'path/to/gene.gtf' star_index = 'path/to/star/index' example.mapClusters(\"merged\", outdir = output, geneGTF = gene_gtf, starIndex = star_index) 6) TE quantification This step runs TEcounts in multi mode (for details, see TEtranscripts documentation). You can download the appropiate TE GTF file here. output = 'wherever/you/want/the/output' gene_gtf = 'path/to/gene.gtf' te_gtf = 'path/to/te.gtf' example.TEcountsClusters(\"merged\", outdir = output, geneGTF = gene_gtf, teGTF = te_gtf) The output directory now contains a subdirectory called TEcounts/ with samples' subdirectories and each of their clusters TE counts. 7) TE counts normalization Before continuing with the downstream analysis, we need to normalize for samples' sequencing depth and cluster size. We can do this for all samples using example.normalizeTECounts(\"merged\") And that's it! You can now see your final count matrix at the output directory and if you want you can use our plot_TEexpression.R script to plot a UMAP with TE subfamilies expression. Note that if you need to cancel the execution of a step (or the pipeline if you decided to go for that) you will have to wait for all samples to finish the step they are at.","title":"Workflow"},{"location":"workflow/#a-workflow-example","text":"Let's begin with a simple example. Let's say you have two samples of the same tissue, they were sequenced in the same sequencing run and you want to get transposon expression per cell type found in the tissue.","title":"A workflow example"},{"location":"workflow/#registering-samples","text":"The first step is to have the fastq files in a file system such as: data \u2502 \u2514\u2500\u2500\u2500sample1 \u2502 \u2502 sample1Name_L001_I1_001.fastq.gz \u2502 \u2502 sample1Name_L001_R1_001.fastq.gz \u2502 \u2502 sample1Name_L001_R2_001.fastq.gz \u2502 \u2502 sample1Name_L002_I1_001.fastq.gz \u2502 \u2502 sample1Name_L002_R1_001.fastq.gz \u2502 \u2514\u2500 sample1Name_L002_R2_001.fastq.gz \u2514\u2500\u2500\u2500sample2 \u2502 sample2Name_L001_I1_001.fastq.gz \u2502 sample2Name_L001_R1_001.fastq.gz \u2502 sample2Name_L001_R2_001.fastq.gz \u2502 sample2Name_L002_I1_001.fastq.gz \u2502 sample2Name_L002_R1_001.fastq.gz \u2514\u2500 sample2Name_L002_R2_001.fastq.gz We can then register the samples in an experiment object import truster raw_path = [\"/path/to/data/\"] example = truster.experiment(name = \"example\") example.registerSamplesFromPath(raw_path) This will look for this structure of files in the path and create an object \"sample\" per subdirectory in the path, the object will have the name of the subdirectories as ID (i.e. sample1) and the name of the files (i.e. sample1Name) as sample name. The \"sample\" objects are contained to the object \"experiment\" which here we called example . If there was any sample in the data/ folder you didn't want to include, you can unregister it with example.unregisterSample(sampleId = \"sample1\") Similarly, you can register individual samples as example.registerSample(sampleId = \"sample3\", sampleName = \"sample3Name\", rawPath = \"/path/to/sample3/fastqfiles/\")","title":"Registering samples"},{"location":"workflow/#mapping-and-quantification","text":"We now need to run cellranger. TrusTEr has a wrapper for it and can be used to run it for all samples registered in an experiment. In this example: crIndex = '/path/to/cellranger/index' outdir = '/output/path' example.quantify(crIndex, outdir) If you already have the output from cellranger, feel free to set the quantification outdir with example.setQuantificationOutdir(outdir) Note: Before continuing, don't forget to check the quality of your samples!","title":"Mapping and quantification"},{"location":"workflow/#clustering","text":"Clustering samples is not a trivial step. It requires knowledge of the tissue being analyzed. We wrapped a very basic clustering with Seurat as a reference to make the process smoother for the user, but be aware that there are plenty of parameters that you could tweak to get the perfect clustering for your particular data.","title":"Clustering"},{"location":"workflow/#per-sample","text":"We can get a clustering of each sample by typing clusters_dir = 'wherever/you/want/the/output' example.getClustersAllSamples(clusters_dir) This will create the clusters_dir and a subfolder per sample (named with the sampleIds ). Each of these subfoldes will contain a tsv file per cluster found on the sample. The tsv files contain the cells barcodes that form that cluster. You will also get an rds file in the output directory with the Seurat object of each of your samples. If you already have a clustering of your preference, please produce the required tsv files and set the clusters directory as: example.setClustersOutdir(processClustersOutdir = outdir) We could add a function in the near future that takes rds with a Seurat object and produces the tsv files in the file structure we need it.","title":"Per sample"},{"location":"workflow/#merged-samples","text":"In some experiments, such as this one where the samples are from the same tissue, it's interesting to combine the samples and get a merged clustering. This can be achieved as mergedsamples_dir = 'wherever/you/want/the/output' example.mergeSamples(mergedsamples_dir) Similarly to the clusters per sample, if you already have a clustering you want to use, you could set the directory where you contain the tsv files with the cell barcodes as example.setMergeSamplesOutdir(outdir) If you are going to merge the Seurat objects yourself, we ask you to name the merged tsv files as [sampleId]_merged.clusters_[cluster number].tsv","title":"Merged samples"},{"location":"workflow/#transposon-expression","text":"Once we have the cellranger output, and the clusters that we want to check for transposon expression, we can go ahead and run trusTEr as a pipeline or step by step. This part is a set of seven steps: Extract cell barcodes from the bam files Filter for unique UMIs Convert bam to fastq files Concatenate lanes from the step #3 output Merge samples in one cluster Map fastq files TE quantification TE quantification normalization","title":"Transposon expression"},{"location":"workflow/#as-a-pipeline","text":"You can run everything at once. You will need to provide a gene and a TE GTF (as required by TEtranscripts output. You can check their downloads here), the path to the STAR index you want to use, the mode meaning the type of clustering you are using (\"merged\" or \"perSample\"), and the output directory. output = 'wherever/you/want/the/output' gene_gtf = 'path/to/gene.gtf' te_gtf = 'path/to/te.gtf' star_index = 'path/to/star/index' example.processClusters(mode = \"merged\", outdir = output, geneGTF = gene_gtf, teGTF = te_gtf, starIndex = star_index)","title":"As a pipeline"},{"location":"workflow/#step-by-step","text":"If you want to wait a bit and check the output of each step of the pipeline, you an of course run it step by step.","title":"Step by step"},{"location":"workflow/#1-extract-clusters","text":"Extract cell barcodes from the bam files typing example.tsvToBamClusters(mode = \"merged\", outdir) This will create a directory inside the outdir named tsvToBam/ . This directory will contain subdirectories one per sample, containing the bam files for each of their clusters.","title":"1) Extract clusters"},{"location":"workflow/#2-filter-umis","text":"Because PCR duplication is a needed step for 10x RNA sequencing, we need to make sure there are no duplicated molecules when quantifying repetitive elements. We ensure the molecules are unique by keeping only reads with a unique combination of cell barcode, UMI and sequence. To filter duplicates in our merged clustering bam files: example.filterUMIsClusters(\"merged\", outdir) This will create a directory inside the outdir named filterUMIs/ . Similarly to tsvToBam/ it contains a subdirectory per sample, each containing the filtered bam file.","title":"2) Filter UMIs"},{"location":"workflow/#3-convert-bam-to-fastq","text":"This is just a file conversion step dependent on bamtofastq from 10x Genomics. example.bamToFastqClusters(\"merged\", outdir) Will create a directory inside the outdir named bamToFastq/ . Each sample subdirectory has subdirectories for each cluster which contain the fastq files of the clusters in different lanes (L00[1-9]).","title":"3) Convert bam to fastq"},{"location":"workflow/#4-concatenate-lanes","text":"We will take the sequence fastq files ( *_R2_001.fastq.gz ) and concatenate them. This will produce a bulk file for each cluster. example.concatenateLanesClusters(\"merged\", outdir) Again, this will create a directory inside the outdir named concatenateLanes/ which will include sample subdirectories containing the concatenated fastq files per cluster.","title":"4) Concatenate lanes"},{"location":"workflow/#5-map-fastq-files","text":"We will now map these files using STAR. Again, this is just a basic wrapper using TEtranscript's authors recommendations, but if you need to tweak more parameters, feel free to do this step yourself and then setting the output directory to continue. You will need a gene GTF and a STAR index. output = 'wherever/you/want/the/output' gene_gtf = 'path/to/gene.gtf' star_index = 'path/to/star/index' example.mapClusters(\"merged\", outdir = output, geneGTF = gene_gtf, starIndex = star_index)","title":"5) Map fastq files"},{"location":"workflow/#6-te-quantification","text":"This step runs TEcounts in multi mode (for details, see TEtranscripts documentation). You can download the appropiate TE GTF file here. output = 'wherever/you/want/the/output' gene_gtf = 'path/to/gene.gtf' te_gtf = 'path/to/te.gtf' example.TEcountsClusters(\"merged\", outdir = output, geneGTF = gene_gtf, teGTF = te_gtf) The output directory now contains a subdirectory called TEcounts/ with samples' subdirectories and each of their clusters TE counts.","title":"6) TE quantification"},{"location":"workflow/#7-te-counts-normalization","text":"Before continuing with the downstream analysis, we need to normalize for samples' sequencing depth and cluster size. We can do this for all samples using example.normalizeTECounts(\"merged\") And that's it! You can now see your final count matrix at the output directory and if you want you can use our plot_TEexpression.R script to plot a UMAP with TE subfamilies expression. Note that if you need to cancel the execution of a step (or the pipeline if you decided to go for that) you will have to wait for all samples to finish the step they are at.","title":"7) TE counts normalization"}]}